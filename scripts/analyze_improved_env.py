#!/usr/bin/env python3
"""
æ”¹è¿›ç¯å¢ƒåˆ†æè„šæœ¬
æ·±åº¦åˆ†æImprovedFlightBookingEnvçš„å¥–åŠ±åˆ†è§£å’ŒæŠ€èƒ½ç³»ç»Ÿ
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from test_time_gym.envs.improved_flight_booking_env import ImprovedFlightBookingEnv


class ImprovedEnvAnalyzer:
    """æ”¹è¿›ç¯å¢ƒåˆ†æå™¨"""

    def __init__(self):
        self.results = {}

    def run_analysis_suite(self):
        """è¿è¡Œå®Œæ•´åˆ†æå¥—ä»¶"""
        print("ğŸ”¬ æ”¹è¿›ç¯å¢ƒæ·±åº¦åˆ†æ")
        print("=" * 60)

        # 1. å¥–åŠ±åˆ†è§£åˆ†æ
        self.analyze_reward_breakdown()

        # 2. éš¾åº¦çº§åˆ«å¯¹æ¯”
        self.analyze_difficulty_levels()

        # 3. æŠ€èƒ½æŒ‡æ ‡æ¼”è¿›
        self.analyze_skill_metrics()

        # 4. ç¡®å®šæ€§éªŒè¯
        self.verify_determinism()

        # 5. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self.generate_visualizations()

        print("\nâœ… åˆ†æå®Œæˆï¼æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Š")

    def analyze_reward_breakdown(self):
        """åˆ†æå¥–åŠ±åˆ†è§£ç³»ç»Ÿ"""
        print("\nğŸ“Š åˆ†æå¥–åŠ±åˆ†è§£ç³»ç»Ÿ...")

        env = ImprovedFlightBookingEnv(seed=42, config={"difficulty": "medium"})

        reward_data = {
            'base_action': [],
            'progress': [],
            'constraint_satisfaction': [],
            'efficiency': [],
            'optimization': [],
            'penalty': [],
            'total': []
        }

        episode_rewards = []

        # è¿è¡Œå¤šä¸ªepisodeæ”¶é›†æ•°æ®
        for episode in range(20):
            obs, info = env.reset()
            episode_reward_breakdown = []

            # æ¨¡æ‹Ÿæ™ºèƒ½ä½“æ‰§è¡Œç­–ç•¥
            actions = [
                "search_flights",
                "filter_results",
                "select_flight cheapest",
                "add_to_cart",
                "proceed_to_payment",
                "enter_card",
                "confirm_payment"
            ]

            for action in actions:
                if env.done or env.truncated:
                    break

                obs, reward, done, trunc, info = env.step(action)

                if 'reward_breakdown' in info:
                    breakdown = info['reward_breakdown']
                    episode_reward_breakdown.append(breakdown)

                    # æ”¶é›†æ•°æ®
                    for key in reward_data:
                        reward_data[key].append(breakdown.get(key, 0))

            episode_rewards.append(episode_reward_breakdown)

        # åˆ†æç»“æœ
        self.results['reward_breakdown'] = {
            'component_stats': {
                component: {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                } for component, values in reward_data.items() if values
            },
            'raw_data': reward_data
        }

        print(f"  â€¢ æ”¶é›†äº† {len([v for v in reward_data['total'] if v != 0])} ä¸ªæœ‰æ•ˆå¥–åŠ±æ ·æœ¬")
        print(f"  â€¢ å¹³å‡æ€»å¥–åŠ±: {np.mean([v for v in reward_data['total'] if v != 0]):.3f}")

    def analyze_difficulty_levels(self):
        """åˆ†æä¸åŒéš¾åº¦çº§åˆ«"""
        print("\nğŸ¯ åˆ†æéš¾åº¦çº§åˆ«å·®å¼‚...")

        difficulties = ["easy", "medium", "hard"]
        difficulty_results = {}

        for difficulty in difficulties:
            env = ImprovedFlightBookingEnv(
                seed=42,
                config={"difficulty": difficulty, "max_steps": 25}
            )

            success_rates = []
            avg_rewards = []
            avg_steps = []
            constraint_violations = []

            for episode in range(15):
                obs, info = env.reset()
                total_reward = 0
                steps = 0

                # ä½¿ç”¨å¹³è¡¡ç­–ç•¥
                while not env.done and not env.truncated and steps < 20:
                    available_actions = obs.get('available_actions', ['restart'])

                    if available_actions:
                        action = available_actions[0]
                    else:
                        action = 'restart'

                    obs, reward, done, trunc, info = env.step(action)
                    total_reward += reward
                    steps += 1

                success = env.done and not env.truncated
                success_rates.append(1 if success else 0)
                avg_rewards.append(total_reward)
                avg_steps.append(steps)

                if 'skill_metrics' in info:
                    constraint_violations.append(info['skill_metrics'].get('constraint_violations', 0))

            difficulty_results[difficulty] = {
                'success_rate': np.mean(success_rates),
                'avg_reward': np.mean(avg_rewards),
                'avg_steps': np.mean(avg_steps),
                'constraint_violations': np.mean(constraint_violations) if constraint_violations else 0,
                'task_complexity': len(env.current_task['constraints'])
            }

            print(f"  â€¢ {difficulty}: æˆåŠŸç‡={np.mean(success_rates):.3f}, å¹³å‡å¥–åŠ±={np.mean(avg_rewards):.3f}")

        self.results['difficulty_analysis'] = difficulty_results

    def analyze_skill_metrics(self):
        """åˆ†ææŠ€èƒ½æŒ‡æ ‡ç³»ç»Ÿ"""
        print("\nğŸ§  åˆ†ææŠ€èƒ½æŒ‡æ ‡ç³»ç»Ÿ...")

        env = ImprovedFlightBookingEnv(seed=42, config={"difficulty": "medium"})

        skill_evolution = {
            'search_efficiency': [],
            'budget_efficiency': [],
            'constraint_violations': [],
            'error_recovery_count': []
        }

        for episode in range(25):
            obs, info = env.reset()

            # æ¨¡æ‹Ÿæ™ºèƒ½ä½“å­¦ä¹ è¿‡ç¨‹
            actions = ["search_flights", "filter_results", "select_flight cheapest",
                      "add_to_cart", "proceed_to_payment", "enter_card", "confirm_payment"]

            for action in actions:
                if env.done or env.truncated:
                    break
                obs, reward, done, trunc, info = env.step(action)

            # æ”¶é›†æŠ€èƒ½æŒ‡æ ‡
            if 'skill_metrics' in info:
                metrics = info['skill_metrics']
                for key in skill_evolution:
                    if key in metrics:
                        skill_evolution[key].append(metrics[key])

        self.results['skill_metrics'] = skill_evolution
        print(f"  â€¢ è·Ÿè¸ªäº† {len(skill_evolution['search_efficiency'])} ä¸ªepisodeçš„æŠ€èƒ½æ¼”è¿›")

    def verify_determinism(self):
        """éªŒè¯ç¡®å®šæ€§"""
        print("\nğŸ” éªŒè¯ç¯å¢ƒç¡®å®šæ€§...")

        # ä½¿ç”¨ç›¸åŒç§å­è¿è¡Œå¤šæ¬¡
        results_run1 = []
        results_run2 = []

        for run in range(2):
            env = ImprovedFlightBookingEnv(seed=123, config={"difficulty": "medium"})
            run_results = []

            for episode in range(5):
                obs, info = env.reset()
                episode_data = {
                    'initial_task': env.current_task.copy(),
                    'flights_count': len(env.flights_db),
                    'rewards': []
                }

                actions = ["search_flights", "filter_results", "select_flight cheapest"]
                for action in actions:
                    if env.done or env.truncated:
                        break
                    obs, reward, done, trunc, info = env.step(action)
                    episode_data['rewards'].append(reward)

                run_results.append(episode_data)

            if run == 0:
                results_run1 = run_results
            else:
                results_run2 = run_results

        # æ£€æŸ¥ä¸€è‡´æ€§
        deterministic = True
        for i in range(len(results_run1)):
            if (results_run1[i]['initial_task'] != results_run2[i]['initial_task'] or
                results_run1[i]['flights_count'] != results_run2[i]['flights_count']):
                deterministic = False
                break

        self.results['determinism'] = {
            'is_deterministic': deterministic,
            'run1_sample': results_run1[0] if results_run1 else None,
            'run2_sample': results_run2[0] if results_run2 else None
        }

        print(f"  â€¢ ç¡®å®šæ€§éªŒè¯: {'âœ… é€šè¿‡' if deterministic else 'âŒ å¤±è´¥'}")

    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "logs/improved_env_analysis"
        os.makedirs(output_dir, exist_ok=True)

        # è®¾ç½®è‹±æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        # 1. å¥–åŠ±åˆ†è§£é¥¼å›¾
        if 'reward_breakdown' in self.results:
            self._plot_reward_breakdown(output_dir)

        # 2. éš¾åº¦çº§åˆ«å¯¹æ¯”
        if 'difficulty_analysis' in self.results:
            self._plot_difficulty_comparison(output_dir)

        # 3. æŠ€èƒ½æŒ‡æ ‡æ¼”è¿›
        if 'skill_metrics' in self.results:
            self._plot_skill_evolution(output_dir)

        # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_report(output_dir)

        print(f"  â€¢ å›¾è¡¨ä¿å­˜è‡³: {output_dir}/")

    def _plot_reward_breakdown(self, output_dir: str):
        """ç»˜åˆ¶å¥–åŠ±åˆ†è§£å›¾"""
        reward_stats = self.results['reward_breakdown']['component_stats']

        # æå–å¹³å‡å€¼
        components = []
        values = []
        for comp, stats in reward_stats.items():
            if comp != 'total' and stats['mean'] != 0:
                components.append(comp)
                values.append(abs(stats['mean']))  # ä½¿ç”¨ç»å¯¹å€¼ç”¨äºé¥¼å›¾

        if values:
            plt.figure(figsize=(10, 8))
            colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc', '#c2c2f0']

            plt.subplot(2, 2, 1)
            plt.pie(values, labels=components, autopct='%1.1f%%', colors=colors[:len(values)])
            plt.title('Reward Component Distribution')

            # å¥–åŠ±ç»„ä»¶æ¡å½¢å›¾
            plt.subplot(2, 2, 2)
            means = [reward_stats[comp]['mean'] for comp in components]
            stds = [reward_stats[comp]['std'] for comp in components]

            bars = plt.bar(range(len(components)), means, yerr=stds, capsize=5)
            plt.xticks(range(len(components)), components, rotation=45)
            plt.title('Reward Component Mean Â± Std')
            plt.ylabel('Reward Value')

            # ä¸ºæ­£è´Ÿå€¼è®¾ç½®ä¸åŒé¢œè‰²
            for i, bar in enumerate(bars):
                if means[i] >= 0:
                    bar.set_color('#66b3ff')
                else:
                    bar.set_color('#ff9999')

            plt.tight_layout()
            plt.savefig(f"{output_dir}/reward_breakdown.png", dpi=300, bbox_inches='tight')
            plt.close()

    def _plot_difficulty_comparison(self, output_dir: str):
        """ç»˜åˆ¶éš¾åº¦çº§åˆ«å¯¹æ¯”å›¾"""
        diff_data = self.results['difficulty_analysis']

        difficulties = list(diff_data.keys())
        metrics = ['success_rate', 'avg_reward', 'avg_steps', 'constraint_violations']

        plt.figure(figsize=(15, 10))

        for i, metric in enumerate(metrics):
            plt.subplot(2, 2, i+1)
            values = [diff_data[diff][metric] for diff in difficulties]

            bars = plt.bar(difficulties, values, color=['#90EE90', '#FFB347', '#FF6B6B'])
            plt.title(f'{metric.replace("_", " ").title()}')
            plt.ylabel('Value')

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(f"{output_dir}/difficulty_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_skill_evolution(self, output_dir: str):
        """ç»˜åˆ¶æŠ€èƒ½æ¼”è¿›å›¾"""
        skill_data = self.results['skill_metrics']

        plt.figure(figsize=(12, 8))

        for i, (skill, values) in enumerate(skill_data.items()):
            if values:  # åªç»˜åˆ¶æœ‰æ•°æ®çš„æŠ€èƒ½
                plt.subplot(2, 2, i+1)
                episodes = range(len(values))
                plt.plot(episodes, values, marker='o', linewidth=2, markersize=4)
                plt.title(f'{skill.replace("_", " ").title()}')
                plt.xlabel('Episode')
                plt.ylabel('Value')
                plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(f"{output_dir}/skill_evolution.png", dpi=300, bbox_inches='tight')
        plt.close()

    def _generate_report(self, output_dir: str):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        report = [
            "ğŸ”¬ æ”¹è¿›ç¯å¢ƒåˆ†ææŠ¥å‘Š",
            "=" * 60,
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            "",
            "ğŸ“Š æ ¸å¿ƒå‘ç°:",
            ""
        ]

        # å¥–åŠ±åˆ†è§£åˆ†æ
        if 'reward_breakdown' in self.results:
            stats = self.results['reward_breakdown']['component_stats']
            report.extend([
                "1. å¥–åŠ±åˆ†è§£ç³»ç»Ÿåˆ†æ:",
                f"   â€¢ ä¸»è¦å¥–åŠ±ç»„ä»¶: {len(stats)} ä¸ª",
                f"   â€¢ å¹³å‡æ€»å¥–åŠ±: {stats.get('total', {}).get('mean', 0):.3f}",
                ""
            ])

        # éš¾åº¦çº§åˆ«åˆ†æ
        if 'difficulty_analysis' in self.results:
            diff_data = self.results['difficulty_analysis']
            report.extend([
                "2. éš¾åº¦çº§åˆ«åˆ†æ:",
            ])
            for diff, data in diff_data.items():
                report.append(f"   â€¢ {diff}: æˆåŠŸç‡={data['success_rate']:.3f}, "
                            f"å¹³å‡å¥–åŠ±={data['avg_reward']:.3f}")
            report.append("")

        # ç¡®å®šæ€§éªŒè¯
        if 'determinism' in self.results:
            is_det = self.results['determinism']['is_deterministic']
            report.extend([
                "3. ç¡®å®šæ€§éªŒè¯:",
                f"   â€¢ ç¯å¢ƒç¡®å®šæ€§: {'âœ… é€šè¿‡' if is_det else 'âŒ å¤±è´¥'}",
                ""
            ])

        # æŠ€èƒ½ç³»ç»Ÿ
        if 'skill_metrics' in self.results:
            skill_data = self.results['skill_metrics']
            report.extend([
                "4. æŠ€èƒ½æŒ‡æ ‡ç³»ç»Ÿ:",
                f"   â€¢ è·Ÿè¸ªæŒ‡æ ‡: {len(skill_data)} ä¸ª",
                f"   â€¢ æ•°æ®å®Œæ•´æ€§: {len([v for v in skill_data.values() if v])} / {len(skill_data)}",
                ""
            ])

        report.extend([
            "ğŸ“ ç”Ÿæˆæ–‡ä»¶:",
            "   â€¢ reward_breakdown.png - å¥–åŠ±åˆ†è§£å¯è§†åŒ–",
            "   â€¢ difficulty_comparison.png - éš¾åº¦çº§åˆ«å¯¹æ¯”",
            "   â€¢ skill_evolution.png - æŠ€èƒ½æ¼”è¿›æ›²çº¿",
            "   â€¢ analysis_report.txt - è¯¦ç»†åˆ†ææŠ¥å‘Š",
            "",
            "âœ¨ åˆ†æå®Œæˆï¼"
        ])

        # ä¿å­˜æŠ¥å‘Š
        with open(f"{output_dir}/analysis_report.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        # ä¿å­˜åŸå§‹æ•°æ®
        with open(f"{output_dir}/raw_data.json", 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = ImprovedEnvAnalyzer()
    analyzer.run_analysis_suite()

    print("\nğŸ“‹ åˆ†ææ€»ç»“:")
    print("âœ… å¥–åŠ±åˆ†è§£ç³»ç»Ÿ - æä¾›è¯¦ç»†çš„å¤šç»´åº¦åé¦ˆ")
    print("âœ… éš¾åº¦åˆ†çº§ç³»ç»Ÿ - æ”¯æŒæ¸è¿›å¼å­¦ä¹ ")
    print("âœ… æŠ€èƒ½æŒ‡æ ‡è·Ÿè¸ª - é‡åŒ–å­¦ä¹ è¿›å±•")
    print("âœ… ç¡®å®šæ€§ä¸šåŠ¡é€»è¾‘ - å‡å°‘éšæœºæ€§ï¼Œæé«˜å¯é‡ç°æ€§")
    print("\nğŸ¯ æ”¹è¿›ç¯å¢ƒå·²å‡†å¤‡å°±ç»ªï¼Œå¯ç”¨äºLLMæ™ºèƒ½ä½“è®­ç»ƒï¼")


if __name__ == "__main__":
    main()
