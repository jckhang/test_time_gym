#!/usr/bin/env python3
"""
æ— ç›‘ç£ç»éªŒç§¯ç´¯å®éªŒè¿è¡Œè„šæœ¬
"""

import asyncio
import argparse
import os
import sys
import json
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from experiment_framework import ExperimentRunner

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/experiment.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def load_experiment_config(config_path: str = "experiment_config.json") -> dict:
    """åŠ è½½å®éªŒé…ç½®"""
    default_config = {
        "experiments": {
            "quick_test": {
                "num_episodes": 50,
                "models": ["gpt-3.5-turbo"],
                "strategies": ["balanced"],
                "description": "å¿«é€Ÿæµ‹è¯•å®éªŒ"
            },
            "full_comparison": {
                "num_episodes": 200,
                "models": ["gpt-3.5-turbo"],
                "strategies": ["balanced", "aggressive", "conservative"],
                "description": "å®Œæ•´å¯¹æ¯”å®éªŒ"
            },
            "multi_model": {
                "num_episodes": 150,
                "models": ["gpt-3.5-turbo", "gpt-4"],
                "strategies": ["balanced"],
                "description": "å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ"
            }
        },
        "general_settings": {
            "results_dir": "logs/experiments",
            "save_intermediate": True,
            "generate_visualizations": True,
            "verbose": True
        }
    }
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except Exception as e:
            logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        logger.info(f"åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
    
    return default_config


async def run_single_experiment(experiment_name: str, config: dict):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    logger.info(f"å¼€å§‹å®éªŒ: {experiment_name}")
    logger.info(f"å®éªŒæè¿°: {config.get('description', 'æ— æè¿°')}")
    
    # åˆ›å»ºå®éªŒç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = f"logs/experiments/{experiment_name}_{timestamp}"
    os.makedirs(experiment_dir, exist_ok=True)
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(experiment_dir)
    
    try:
        # è¿è¡Œå®éªŒ
        results = await runner.run_comparative_experiment(
            num_episodes=config["num_episodes"],
            models=config["models"],
            strategies=config["strategies"]
        )
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = os.path.join(experiment_dir, "complete_results.json")
        
        # ç®€åŒ–ç»“æœä»¥ä¾¿ä¿å­˜
        simplified_results = {}
        for exp_name, exp_data in results["experiment_results"].items():
            simplified_results[exp_name] = {
                "config": exp_data["config"],
                "final_stats": exp_data["final_stats"],
                "learning_stats": exp_data["learning_stats"],
                "num_episodes": len(exp_data["episode_results"])
            }
        
        save_data = {
            "experiment_name": experiment_name,
            "timestamp": timestamp,
            "config": config,
            "results": simplified_results,
            "comparison_report": results["comparison_report"]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è§†åŒ–
        if config.get("generate_visualizations", True):
            runner.visualize_results(results["experiment_results"], experiment_dir)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print_results_summary(results["comparison_report"])
        
        logger.info(f"å®éªŒ {experiment_name} å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {experiment_dir}")
        return results
        
    except Exception as e:
        logger.error(f"å®éªŒ {experiment_name} å¤±è´¥: {e}")
        raise


def print_results_summary(comparison_report: dict):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "="*80)
    print("å®éªŒç»“æœæ‘˜è¦")
    print("="*80)
    
    overall = comparison_report.get("overall_improvement", {})
    
    if overall:
        print(f"ğŸ“Š æµ‹è¯•é…ç½®æ•°é‡: {overall['configurations_tested']}")
        print(f"ğŸ“ˆ æœ‰æ”¹è¿›çš„é…ç½®: {overall['improvements_positive']}")
        print(f"ğŸ¯ å¹³å‡æˆåŠŸç‡æ”¹è¿›: {overall['avg_success_rate_improvement']:.3f}")
        print(f"âš¡ å¹³å‡æ­¥æ•°æ”¹è¿›: {overall['avg_steps_improvement']:.3f}")
        print(f"ğŸª å¹³å‡ç¨³å®šæ€§æ”¹è¿›: {overall['avg_stability_improvement']:.3f}")
        
        print("\nğŸ“‹ è¯¦ç»†é…ç½®ç»“æœ:")
        for config, details in comparison_report["detailed_comparisons"].items():
            print(f"\n  ğŸ”§ {config}:")
            print(f"    æˆåŠŸç‡: {details['baseline_success_rate']:.3f} â†’ {details['experience_success_rate']:.3f} "
                  f"({'ğŸ”¥+' if details['success_rate_improvement'] > 0 else 'â„ï¸'}{details['success_rate_improvement']:.3f})")
            print(f"    å¹³å‡æ­¥æ•°: {details['baseline_avg_steps']:.1f} â†’ {details['experience_avg_steps']:.1f} "
                  f"({'âš¡-' if details['steps_improvement'] > 0 else 'ğŸŒ+'}{abs(details['steps_improvement']):.1f})")
            print(f"    å­¦åˆ°æŠ€èƒ½æ•°: {details['total_skills_learned']} ä¸ª")
            print(f"    æŠ€èƒ½ä½¿ç”¨ç‡: {details['skill_usage_rate']:.3f}")
        
        # è¯„ä¼°ç»“æœ
        print("\nğŸ–ï¸ å®éªŒè¯„ä¼°:")
        if overall['avg_success_rate_improvement'] > 0.05:
            print("  âœ… ç»éªŒå­¦ä¹ æ˜¾è‘—æå‡äº†æˆåŠŸç‡!")
        elif overall['avg_success_rate_improvement'] > 0.01:
            print("  âœ´ï¸ ç»éªŒå­¦ä¹ é€‚åº¦æå‡äº†æˆåŠŸç‡")
        else:
            print("  âŒ ç»éªŒå­¦ä¹ å¯¹æˆåŠŸç‡æ”¹è¿›æœ‰é™")
        
        if overall['avg_steps_improvement'] > 1.0:
            print("  âœ… ç»éªŒå­¦ä¹ æ˜¾è‘—æå‡äº†æ•ˆç‡!")
        elif overall['avg_steps_improvement'] > 0.5:
            print("  âœ´ï¸ ç»éªŒå­¦ä¹ é€‚åº¦æå‡äº†æ•ˆç‡")
        else:
            print("  âŒ ç»éªŒå­¦ä¹ å¯¹æ•ˆç‡æ”¹è¿›æœ‰é™")
    
    print("\n" + "="*80)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡Œæ— ç›‘ç£ç»éªŒç§¯ç´¯å®éªŒ")
    parser.add_argument("--experiment", "-e", 
                       choices=["quick_test", "full_comparison", "multi_model", "all"],
                       default="quick_test",
                       help="é€‰æ‹©å®éªŒç±»å‹")
    parser.add_argument("--config", "-c", 
                       default="experiment_config.json",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", 
                       action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = load_experiment_config(args.config)
    
    if args.experiment == "all":
        # è¿è¡Œæ‰€æœ‰å®éªŒ
        for exp_name in config["experiments"]:
            try:
                await run_single_experiment(exp_name, config["experiments"][exp_name])
                print(f"\n{'='*50}\n")
            except Exception as e:
                logger.error(f"å®éªŒ {exp_name} å¤±è´¥: {e}")
    else:
        # è¿è¡ŒæŒ‡å®šå®éªŒ
        if args.experiment in config["experiments"]:
            await run_single_experiment(args.experiment, config["experiments"][args.experiment])
        else:
            logger.error(f"æœªæ‰¾åˆ°å®éªŒé…ç½®: {args.experiment}")
            return
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(main())