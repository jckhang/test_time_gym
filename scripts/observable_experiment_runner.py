#!/usr/bin/env python3
"""
å¯è§‚æµ‹çš„å®éªŒè¿è¡Œå™¨
é›†æˆè§‚æµ‹ç³»ç»Ÿåˆ°å®éªŒæ¡†æ¶ä¸­
"""

import asyncio
import logging
import os
import time
from typing import Dict, List, Optional, Any
from datetime import datetime

# é¡¹ç›®å¯¼å…¥
from experiment_framework import ExperimentRunner, ExperienceEnhancedAgent
from observation_system import ObservationSystem
from test_time_gym.envs.flight_booking_env import FlightBookingEnv
from enhanced_skill_system import EnhancedSkillManager


class ObservableSkillManager(EnhancedSkillManager):
    """å¯è§‚æµ‹çš„æŠ€èƒ½ç®¡ç†å™¨"""
    
    def __init__(self, storage_dir: str, observation_system: Optional[ObservationSystem] = None):
        super().__init__(storage_dir)
        self.observation_system = observation_system
        self.experiment_name = storage_dir.split('/')[-1] if '/' in storage_dir else storage_dir
    
    def process_episode(self, trajectory: List[Dict], final_reward: float, episode_id: str):
        """å¤„ç†episodeå¹¶å‘é€è§‚æµ‹äº‹ä»¶"""
        # è°ƒç”¨åŸå§‹å¤„ç†é€»è¾‘
        super().process_episode(trajectory, final_reward, episode_id)
        
        # å‘é€è§‚æµ‹äº‹ä»¶
        if self.observation_system:
            # è®°å½•æŠ€èƒ½å­¦ä¹ äº‹ä»¶
            for skill_id, skill in self.skills.items():
                if hasattr(skill, '_just_updated'):  # æ ‡è®°åˆšæ›´æ–°çš„æŠ€èƒ½
                    self.observation_system.log_skill_learned(
                        self.experiment_name,
                        skill.name,
                        skill.success_rate.mean(),
                        skill.usage_count
                    )
                    delattr(skill, '_just_updated')
    
    def select_skill_with_exploration(self, observation: Dict):
        """é€‰æ‹©æŠ€èƒ½å¹¶è®°å½•ä½¿ç”¨"""
        selected_skill = super().select_skill_with_exploration(observation)
        
        # è®°å½•æŠ€èƒ½ä½¿ç”¨
        if selected_skill and self.observation_system:
            self.observation_system.log_skill_usage(
                self.experiment_name,
                selected_skill.name,
                True  # è¿™é‡Œç®€åŒ–ä¸ºTrueï¼Œå®é™…åº”è¯¥æ ¹æ®åç»­ç»“æœåˆ¤æ–­
            )
        
        return selected_skill
    
    def update_skill(self, skill_id: str, success: bool, context: Dict = None):
        """æ›´æ–°æŠ€èƒ½å¹¶æ ‡è®°ä¸ºåˆšæ›´æ–°"""
        # è°ƒç”¨åŸå§‹æ›´æ–°é€»è¾‘
        if hasattr(super(), 'update_skill'):
            super().update_skill(skill_id, success, context)
        
        # æ ‡è®°ä¸ºåˆšæ›´æ–°
        if skill_id in self.skills:
            self.skills[skill_id]._just_updated = True


class ObservableAgent(ExperienceEnhancedAgent):
    """å¯è§‚æµ‹çš„æ™ºèƒ½ä½“"""
    
    def __init__(self, model: str = None, strategy: str = "balanced", 
                 skill_manager: Optional[ObservableSkillManager] = None,
                 use_experience: bool = True,
                 observation_system: Optional[ObservationSystem] = None,
                 experiment_name: str = "unknown"):
        super().__init__(model, strategy, skill_manager, use_experience)
        self.observation_system = observation_system
        self.experiment_name = experiment_name
        self.current_episode_id = None
        self.step_count = 0
    
    def start_episode(self, episode_id: str, initial_obs: Dict):
        """å¼€å§‹æ–°çš„episode"""
        self.current_episode_id = episode_id
        self.step_count = 0
        
        if self.observation_system:
            self.observation_system.log_episode_start(
                self.experiment_name, 
                episode_id, 
                initial_obs
            )
    
    async def select_action(self, observation: Dict[str, Any]) -> str:
        """é€‰æ‹©åŠ¨ä½œå¹¶è®°å½•è§‚æµ‹"""
        # è°ƒç”¨åŸå§‹åŠ¨ä½œé€‰æ‹©
        action = await super().select_action(observation)
        
        # è®°å½•æ­¥éª¤ï¼ˆå‡è®¾å¥–åŠ±ä¼šåœ¨åç»­stepä¸­ç»™å‡ºï¼‰
        if self.observation_system and self.current_episode_id:
            skill_used = None
            if (hasattr(self, 'current_trajectory') and 
                self.current_trajectory and 
                self.current_trajectory[-1].get('skill_used')):
                skill_used = self.current_trajectory[-1]['skill_used']
            
            self.observation_system.log_step(
                self.experiment_name,
                self.current_episode_id,
                self.step_count,
                action,
                observation,
                0,  # å¥–åŠ±ç¨åæ›´æ–°
                skill_used=skill_used
            )
            
            self.step_count += 1
        
        return action
    
    def log_step_reward(self, reward: float, error: str = None):
        """è®°å½•æ­¥éª¤å¥–åŠ±"""
        # è¿™æ˜¯ä¸€ä¸ªé¢å¤–çš„æ–¹æ³•æ¥è®°å½•å¥–åŠ±
        if self.observation_system and self.current_episode_id:
            # æ›´æ–°æœ€åä¸€ä¸ªæ­¥éª¤çš„å¥–åŠ±
            pass  # åœ¨å®é™…å®ç°ä¸­å¯ä»¥è€ƒè™‘æ›´æ–°äº‹ä»¶
    
    def end_episode(self, final_reward: float, episode_id: str):
        """ç»“æŸepisodeå¹¶è®°å½•è§‚æµ‹"""
        # è°ƒç”¨åŸå§‹ç»“æŸé€»è¾‘
        super().end_episode(final_reward, episode_id)
        
        # è®°å½•episodeç»“æŸ
        if self.observation_system:
            success = final_reward > 0.5  # ç®€åŒ–çš„æˆåŠŸåˆ¤æ–­
            
            self.observation_system.log_episode_end(
                self.experiment_name,
                self.current_episode_id or episode_id,
                final_reward,
                success,
                self.current_trajectory
            )
        
        # é‡ç½®çŠ¶æ€
        self.current_episode_id = None
        self.step_count = 0


class ObservableExperimentRunner(ExperimentRunner):
    """å¯è§‚æµ‹çš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, results_dir: str = "logs/experiments", 
                 enable_observation: bool = True,
                 web_port: int = 5000):
        super().__init__(results_dir)
        
        # åˆå§‹åŒ–è§‚æµ‹ç³»ç»Ÿ
        self.observation_system = None
        if enable_observation:
            self.observation_system = ObservationSystem(
                enable_web=True, 
                web_port=web_port
            )
        
        self.active_experiments = []
    
    async def run_comparative_experiment(self, 
                                       num_episodes: int = 200,
                                       models: List[str] = None,
                                       strategies: List[str] = None) -> Dict:
        """è¿è¡Œå¯è§‚æµ‹çš„å¯¹æ¯”å®éªŒ"""
        
        if models is None:
            models = ["gpt-3.5-turbo"]
        if strategies is None:
            strategies = ["balanced"]
        
        # å‡†å¤‡å®éªŒåç§°åˆ—è¡¨
        experiment_names = []
        experiment_configs = [
            {"use_experience": False, "name": "baseline"},
            {"use_experience": True, "name": "with_experience"}
        ]
        
        for model in models:
            for strategy in strategies:
                for config in experiment_configs:
                    experiment_name = f"{model}_{strategy}_{config['name']}"
                    experiment_names.append(experiment_name)
        
        # å¯åŠ¨è§‚æµ‹ç³»ç»Ÿ
        if self.observation_system:
            self.observation_system.start_monitoring(experiment_names)
            logging.info("ğŸ”¬ è§‚æµ‹ç³»ç»Ÿå·²å¯åŠ¨")
            logging.info(f"ğŸ“Š Webä»ªè¡¨æ¿: http://localhost:{self.observation_system.web_port}")
        
        # è¿è¡Œå®éªŒ
        results = {}
        
        for model in models:
            for strategy in strategies:
                for config in experiment_configs:
                    experiment_name = f"{model}_{strategy}_{config['name']}"
                    logging.info(f"å¼€å§‹å¯è§‚æµ‹å®éªŒ: {experiment_name}")
                    
                    # è¿è¡Œå®éªŒ
                    experiment_results = await self._run_observable_experiment(
                        model=model,
                        strategy=strategy,
                        use_experience=config["use_experience"],
                        num_episodes=num_episodes,
                        experiment_name=experiment_name
                    )
                    
                    results[experiment_name] = experiment_results
                    
                    # ä¿å­˜ä¸­é—´ç»“æœ
                    self._save_intermediate_results(results)
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        comparison_report = self._generate_comparison_report(results)
        
        # ç”Ÿæˆè§‚æµ‹æŠ¥å‘Š
        if self.observation_system:
            self.observation_system.generate_final_report(
                os.path.join(self.results_dir, "observation_reports")
            )
        
        return {
            "experiment_results": results,
            "comparison_report": comparison_report
        }
    
    async def _run_observable_experiment(self, 
                                       model: str,
                                       strategy: str, 
                                       use_experience: bool,
                                       num_episodes: int,
                                       experiment_name: str) -> Dict:
        """è¿è¡Œå•ä¸ªå¯è§‚æµ‹å®éªŒ"""
        
        # åˆ›å»ºç¯å¢ƒ
        env = FlightBookingEnv(seed=42)
        
        # åˆ›å»ºå¯è§‚æµ‹çš„æŠ€èƒ½ç®¡ç†å™¨
        skill_manager = ObservableSkillManager(
            f"{self.results_dir}/skills_{experiment_name}",
            self.observation_system
        )
        
        # åˆ›å»ºå¯è§‚æµ‹çš„æ™ºèƒ½ä½“
        agent = ObservableAgent(
            model=model,
            strategy=strategy,
            skill_manager=skill_manager,
            use_experience=use_experience,
            observation_system=self.observation_system,
            experiment_name=experiment_name
        )
        
        # å®éªŒç»Ÿè®¡
        episode_results = []
        learning_curves = {
            'success_rate': [],
            'avg_reward': [],
            'avg_steps': [],
            'skill_usage': []
        }
        
        # è¿è¡Œepisodes
        for episode in range(num_episodes):
            episode_id = f"ep_{episode:03d}"
            episode_start_time = time.time()
            
            # é‡ç½®ç¯å¢ƒ
            obs, info = env.reset(seed=42 + episode)
            
            # å¼€å§‹episodeè§‚æµ‹
            agent.start_episode(episode_id, obs)
            
            # å¼€å§‹è®°å½•è½¨è¿¹
            self.trajectory_logger.start_episode(
                agent_type=experiment_name,
                seed=42 + episode,
                initial_obs=obs
            )
            
            total_reward = 0.0
            step_count = 0
            skill_usage_count = 0
            
            # è¿è¡Œepisode
            for step in range(50):  # æœ€å¤§æ­¥æ•°
                try:
                    action = await agent.select_action(obs)
                    
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æŠ€èƒ½
                    if (hasattr(agent, 'current_trajectory') and 
                        agent.current_trajectory and 
                        agent.current_trajectory[-1].get('skill_used')):
                        skill_usage_count += 1
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    obs, reward, done, trunc, info = env.step(action)
                    total_reward += reward
                    step_count += 1
                    
                    # è®°å½•å¥–åŠ±åˆ°è§‚æµ‹ç³»ç»Ÿ
                    agent.log_step_reward(reward)
                    
                    # è®°å½•æ­¥éª¤åˆ°è½¨è¿¹
                    self.trajectory_logger.log_step(
                        step=step,
                        action=action,
                        observation=obs,
                        reward=reward,
                        done=done,
                        info=info
                    )
                    
                    if done or trunc:
                        break
                        
                except Exception as e:
                    logging.error(f"Episode {episode} æ­¥éª¤ {step} å‡ºé”™: {e}")
                    # è®°å½•é”™è¯¯åˆ°è§‚æµ‹ç³»ç»Ÿ
                    if self.observation_system:
                        self.observation_system.log_step(
                            experiment_name, episode_id, step,
                            "error", obs, 0, error=str(e)
                        )
                    break
            
            # ç»“æŸepisode
            episode_duration = time.time() - episode_start_time
            success = obs.get('view') == 'receipt' if 'obs' in locals() else False
            
            # æ›´æ–°æ™ºèƒ½ä½“ç»éªŒ
            agent.end_episode(total_reward, episode_id)
            
            # ç»“æŸè½¨è¿¹è®°å½•
            trajectory_id = self.trajectory_logger.end_episode(total_reward, success)
            
            # è®°å½•episodeç»“æœï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰
            from test_time_gym.utils.evaluation import EpisodeResult
            episode_result = EpisodeResult(
                episode_id=episode_id,
                agent_type=experiment_name,
                seed=42 + episode,
                steps=step_count,
                total_reward=total_reward,
                final_reward=total_reward,
                success=success,
                constraint_violations=0,
                regret=0.0,
                exploration_steps=step_count - skill_usage_count,
                exploitation_steps=skill_usage_count,
                skill_calls=skill_usage_count,
                timestamp=datetime.now().isoformat(),
                trajectory=[]
            )
            
            episode_results.append(episode_result)
            self.metrics.add_episode(episode_result)
            
            # æ›´æ–°å­¦ä¹ æ›²çº¿
            if (episode + 1) % 10 == 0:
                recent_episodes = episode_results[-10:]
                
                learning_curves['success_rate'].append(
                    sum(1 for ep in recent_episodes if ep.success) / len(recent_episodes)
                )
                learning_curves['avg_reward'].append(
                    np.mean([ep.total_reward for ep in recent_episodes])
                )
                learning_curves['avg_steps'].append(
                    np.mean([ep.steps for ep in recent_episodes])
                )
                learning_curves['skill_usage'].append(
                    np.mean([ep.skill_calls for ep in recent_episodes])
                )
            
            # å®šæœŸæŠ¥å‘Šè¿›åº¦
            if (episode + 1) % 25 == 0:
                recent_success_rate = sum(1 for ep in episode_results[-25:] if ep.success) / 25
                logging.info(f"ğŸ”¬ {experiment_name} - Episode {episode + 1}: "
                          f"æœ€è¿‘25æ¬¡æˆåŠŸç‡ {recent_success_rate:.3f}")
                
                # å¦‚æœå¯ç”¨äº†ç»éªŒå­¦ä¹ ï¼ŒæŠ¥å‘ŠæŠ€èƒ½ç»Ÿè®¡
                if use_experience:
                    skill_stats = agent.get_learning_stats()
                    logging.info(f"ğŸ§  æŠ€èƒ½ç»Ÿè®¡: {skill_stats.get('total_skills', 0)} ä¸ªæŠ€èƒ½, "
                              f"å¹³å‡æˆåŠŸç‡ {skill_stats.get('avg_success_rate', 0):.3f}")
        
        # ä¿å­˜æŠ€èƒ½
        if use_experience:
            agent.skill_manager.save_skills()
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        final_stats = self._calculate_experiment_stats(episode_results)
        learning_stats = agent.get_learning_stats() if use_experience else {}
        
        return {
            "config": {
                "model": model,
                "strategy": strategy,
                "use_experience": use_experience,
                "num_episodes": num_episodes
            },
            "episode_results": episode_results,
            "learning_curves": learning_curves,
            "final_stats": final_stats,
            "learning_stats": learning_stats
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.observation_system:
            self.observation_system.cleanup()


# é…ç½®æ–‡ä»¶æ”¯æŒ
class ObservationConfig:
    """è§‚æµ‹é…ç½®"""
    
    @staticmethod
    def load_config(config_path: str = "observation_config.yaml") -> Dict:
        """åŠ è½½è§‚æµ‹é…ç½®"""
        default_config = {
            'observation': {
                'enabled': True,
                'web_dashboard': {
                    'enabled': True,
                    'port': 5000,
                    'host': '0.0.0.0'
                },
                'console_reporting': {
                    'enabled': True,
                    'interval_seconds': 10
                },
                'visualization': {
                    'real_time_plots': True,
                    'trajectory_analysis': True
                },
                'reporting': {
                    'auto_generate': True,
                    'output_dir': 'logs/observation_reports'
                }
            },
            'monitoring': {
                'episode_metrics': ['success_rate', 'avg_reward', 'avg_steps'],
                'skill_metrics': ['learning_rate', 'usage_frequency', 'success_rate'],
                'real_time_threshold': 0.1  # ç§’
            }
        }
        
        if os.path.exists(config_path):
            try:
                import yaml
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                
                # åˆå¹¶é…ç½®
                def merge_dicts(default, user):
                    for key, value in user.items():
                        if isinstance(value, dict) and key in default:
                            merge_dicts(default[key], value)
                        else:
                            default[key] = value
                
                merge_dicts(default_config, user_config)
                return default_config
            except Exception as e:
                logging.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        return default_config


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•° - è¿è¡Œå¯è§‚æµ‹å®éªŒ"""
    
    # åŠ è½½é…ç½®
    config = ObservationConfig.load_config()
    
    # åˆ›å»ºå¯è§‚æµ‹å®éªŒè¿è¡Œå™¨
    runner = ObservableExperimentRunner(
        "logs/observable_experiments",
        enable_observation=config['observation']['enabled'],
        web_port=config['observation']['web_dashboard']['port']
    )
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    print("ğŸš€ å¼€å§‹å¯è§‚æµ‹çš„æ— ç›‘ç£ç»éªŒç§¯ç´¯å¯¹æ¯”å®éªŒ...")
    print("ğŸ“Š æ‰“å¼€æµè§ˆå™¨è®¿é—® http://localhost:5000 æŸ¥çœ‹å®æ—¶ç›‘æ§")
    
    try:
        results = await runner.run_comparative_experiment(
            num_episodes=50,  # ä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨è¾ƒå°‘çš„episodes
            models=["gpt-3.5-turbo"],
            strategies=["balanced"]
        )
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¯ å¯è§‚æµ‹å®éªŒç»“æœæ‘˜è¦")
        print("="*60)
        
        comparison_report = results["comparison_report"]
        overall = comparison_report.get("overall_improvement", {})
        
        if overall:
            print(f"ğŸ“Š æµ‹è¯•é…ç½®æ•°é‡: {overall['configurations_tested']}")
            print(f"ğŸ“ˆ æœ‰æ”¹è¿›çš„é…ç½®: {overall['improvements_positive']}")
            print(f"ğŸ¯ å¹³å‡æˆåŠŸç‡æ”¹è¿›: {overall['avg_success_rate_improvement']:.3f}")
            print(f"âš¡ å¹³å‡æ­¥æ•°æ”¹è¿›: {overall['avg_steps_improvement']:.3f}")
            print(f"ğŸª å¹³å‡ç¨³å®šæ€§æ”¹è¿›: {overall['avg_stability_improvement']:.3f}")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {runner.results_dir}")
        print("ğŸ“Š è§‚æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        runner.cleanup()
        print("ğŸ§¹ æ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main())