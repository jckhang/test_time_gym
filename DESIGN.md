# 《LLM 智能体测试时 Gym 环境设计文档》

## 1. 背景与问题定义

在现实世界中让智能体直接与复杂环境交互（如机票购买、酒店预订、金融交易等）存在以下问题：

- **高风险**：真实操作可能导致资金损失或不可逆错误。
- **高成本**：反复尝试需要真实支付。
- **低可控性**：外部网站随时变化，难以保证实验可复现。

因此，需要一个**可控的仿真环境（Test-Time Gym）**，让智能体能够在安全条件下，探索、总结并复用“程序性知识”（procedural knowledge）。

目标是：

- **支持智能体自主学习流程**（例如订票流程：搜索→筛选→加入购物车→支付）。
- **在无人工干预下探索/利用（explore/exploit）**，并通过经验积累改进策略。
- **提供可量化指标**（成功率、约束满足度、平均步长等），以便调试与评估。

---

## 2. 核心思路

1. **经验存储**
   - 将智能体的交互轨迹记录为结构化数据（含状态、动作、结果、奖励）。
   - 从中挖掘出**可复用的子流程（skills/macro-actions）**。

2. **技能抽取与利用**
   - 通过频繁模式挖掘，将成功轨迹转化为“技能”。
   - 在新任务中，使用 **bandit/meta-RL 策略** 在技能间进行选择与组合。

3. **奖励与反馈机制**
   - 除了任务完成奖励，还设计**内在奖励**（约束满足度、自洽性、预测误差、进展程度）。
   - 避免纯随机探索导致无效尝试。

4. **安全与可控**
   - 全部交互发生在模拟环境中，保证无真实资金流出。
   - 可通过随机扰动（如支付失败、航班售罄）增加鲁棒性。

---

## 3. 系统架构

```
+-----------------------+
|       智能体 (LLM)     |
|-----------------------|
| - 策略/技能选择        |
| - 交互与反思            |
| - 经验存储与更新        |
+-----------+-----------+
            |
            v
+-----------------------+
|   Test-Time Gym 环境   |
|-----------------------|
| 1. 状态生成器           |
| 2. 动作执行器           |
| 3. 奖励计算器           |
| 4. 随机扰动模块         |
| 5. 记录与回放器         |
+-----------+-----------+
            |
            v
+-----------------------+
|     日志与评估模块      |
|-----------------------|
| - 成功率                |
| - 约束违规统计           |
| - 平均步长              |
| - 探索/利用比例         |
+-----------------------+
```

---

## 4. 环境设计

### 4.1 状态（Observation）

使用结构化 JSON 表示，避免 HTML 噪音：

```json
{
  "view": "search_results | cart | payment | receipt | error",
  "forms": {"from": "SFO", "to": "MAD", "date": "2025-10-15"},
  "flights": [
    {"id": "IB6123", "price": 870, "depart": "10:40", "arrive": "07:15+1", "stops": 1, "airline": "IB"}
  ],
  "cart": {"items": [{"flight_id": "IB6123", "price": 870}], "total": 870},
  "payment_state": {"needs_3ds": true, "attempts": 0},
  "constraints": {"budget": 900, "depart_after": "10:00", "max_stops": 1},
  "messages": ["Tip: Prices drop if you shift ±1 day"]
}
```

### 4.2 动作（Action）

环境提供语义级别 API，而不是低级按键操作：

- `search_flights(from, to, date)`
- `filter_results({depart_after, max_stops, airline})`
- `select_flight(flight_id)`
- `add_to_cart(flight_id)`
- `proceed_to_payment()`
- `enter_card(card_token)`
- `confirm_payment()`
- `apply_coupon(code)`
- `restart()` / `abort()`

### 4.3 奖励机制（Reward）

- **终局奖励**：任务完成且满足约束 → +1.0
- **约束违规**：超预算、时间不符等 → −0.3
- **过程奖励**：如成功加购物车、进入支付页面 → +0.02 ~ +0.05
- **惩罚**：无效动作、支付失败 → −0.05
- **时间成本**：每步 −0.01

### 4.4 随机扰动（Stochasticity）

- 随机要求 3DS 验证
- 随机支付失败
- 随机航班售罄
- 航班排序随机化

### 4.5 日志与回放

- 所有 episode 存储为 JSONL 文件（状态、动作、奖励、结果）。
- 可回放用于调试、离线训练与技能挖掘。

---

## 5. 智能体学习机制

### 5.1 技能提取

- 使用序列模式挖掘（PrefixSpan/SPADe）找到高频且成功率高的子流程。
- 将其抽象为可重用的**宏动作（macro skill）**，含前置条件、效果与可靠性。

### 5.2 策略选择

- 使用 **Thompson Sampling / Contextual Bandit** 在技能集合中选择最优候选。
- 无高置信度技能时，回退到基于 LLM 的规划/推理。

### 5.3 内在奖励信号

- **自洽性检查**（JSON 格式正确、约束满足）。
- **进展度量**（购物车非空、距离目标更近）。
- **预测误差/好奇心**（结果与预期差异大 → 激励探索）。

### 5.4 避免遗忘

- 定期蒸馏：将成功率高的流程固化为简洁技能库。
- 弹性记忆：衰减低效技能，保留多样化代表样本。

---

## 6. 评估指标

- **任务成功率**：成功出票且满足约束的比例。
- **平均步长**：完成任务所需的平均交互次数。
- **约束违规率**：未满足预算/时间要求的比例。
- **探索/利用比**：技能调用与新探索的比例。
- **后悔值（Regret）**：实际票价 − 最优票价。

---

## 7. 示例运行流程

1. Agent 接收到任务：订 SFO→MAD 机票，预算 ≤900。
2. 调用 `search_flights()` → 得到结果。
3. 使用 `filter_results({"depart_after":"10:00"})` → 筛选。
4. 执行 `add_to_cart(flight_id)` → 将航班加入购物车。
5. `proceed_to_payment()` → 进入支付页面。
6. `enter_card()` + `confirm_payment()` → 成功出票。
7. 环境奖励 +1.0，记录经验。
8. Agent 在后续类似任务中**直接利用该流程**，减少探索成本。

---

## 8. 实现细节

- 使用 **Gymnasium/Gym API** 实现环境（`reset()`, `step()`, `render()`）。
- 状态与动作均使用 **JSON/结构化文本**，方便 LLM 处理。
- 支持 **随机种子**，保证实验可复现。
- 提供 **离线回放工具**，可用于离线强化学习或技能挖掘。

---

## 9. 总结

本设计文档提出了一个 **测试时 Gym 环境**，其特点是：

- **安全可控**：不涉及真实金钱交易。
- **程序性知识获取**：通过经验挖掘与技能抽取，让智能体学习可重用流程。
- **在线适应**：支持智能体在测试时根据经验进行探索与利用。
- **可评估**：内置多维度指标，便于分析学习效果。

该环境不仅适用于订票场景，还可推广到其他多步骤任务（如购物结账、信息检索、表单填写），是研究 **LLM Agent 自主学习与程序知识获取** 的通用框架。
